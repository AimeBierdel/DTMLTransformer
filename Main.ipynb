{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Imports and Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pipeline \n",
    "import Models \n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torchfitter.utils.convenience import get_logger\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "logger = get_logger(name=\"DTML \")\n",
    "level = logger.level\n",
    "logging.basicConfig(level=level)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameters : \n",
    "#[0] beta (weight of global market correlation)\n",
    "#[1] nglobal : number of lines describing market indexes \n",
    "#[2] h (length of context vector)\n",
    "#[3] window-length\n",
    "#[4] batch_size \n",
    "#[5] number of epochs for training\n",
    "#[6] initial learning rate\n",
    "#[7] n_layers in LSTM\n",
    "#[8] dropout_rate\n",
    "\n",
    "params =torch.tensor([ 0.3 , 1 , 2, 16, 32, 100, 1e-3, 2, 0.1],requires_grad=False).float()\n",
    "\n",
    "### Index of the stock to predict. Warning : this will be the nglobal + stock_to_predict -1 th line of the dataset since we are\n",
    "### not predicting the market indexes used\n",
    "stock_to_predict = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"IMPORTING DATASET\")\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "html_data = requests.get(url).text\n",
    "soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "tickers = []\n",
    "for row in table.findAll(\"tr\")[1:]:\n",
    "    ticker = row.findAll(\"td\")[0].text.strip()\n",
    "    tickers.append(ticker)\n",
    "\n",
    "# Add the S&P 500 ETF ticker (SPY) at the beginning of the list\n",
    "tickers.insert(0, \"SPY\")\n",
    "# Download\n",
    "data = yf.download(tickers = tickers,  \n",
    "            period = \"10y\",         \n",
    "            interval = \"1wk\",       \n",
    "            prepost = False,       \n",
    "            repair = True)  \n",
    "\n",
    "data = data['Close']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"CLEANING DATASET\")\n",
    "data = data.loc[ data.isna().sum(axis = 1) < 500, :]\n",
    "data = data.fillna(method = 'ffill')\n",
    "temp = data.isna().sum(axis = 0)\n",
    "data = data.loc[ :, temp  == 0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convers to Float\n",
    "data = data.astype(dtype=\"float32\")\n",
    "\n",
    "### Get the SPY market index first \n",
    "idx = data.columns.get_loc(\"SPY\")\n",
    "cols = [data.columns.values[idx]]\n",
    "cols  = cols + data.columns.values[0:idx].tolist()\n",
    "cols  =   cols +  data.columns.values[idx+1:len(data.columns.values)].tolist()\n",
    "data = data[cols]\n",
    "### Retrieve ticker - index vector \n",
    "tickers = data.columns.values\n",
    "\n",
    "### get the stock price data into percent change format \n",
    "data = data.pct_change(1)\n",
    "data = data.iloc[1:,:]\n",
    "\n",
    "### Scaling\n",
    "s = np.std(data.values,axis=(0,1))\n",
    "mu = np.mean(data.values, axis = (0,1))\n",
    "data = (data - mu)/s\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_idx = int(data.shape[0] * 0.8)\n",
    "train_df = data.iloc[0:split_idx,:]\n",
    "remaining_df = data.iloc[split_idx: , :]\n",
    "split_idx_2 = int(remaining_df.shape[0] * 0.5)\n",
    "test_df = remaining_df.iloc[0:split_idx_2,:]\n",
    "validation_df = remaining_df.iloc[split_idx_2:, :]\n",
    "\n",
    "### We add a new dimension at the end. The model takes a T x Nstocks x Nfeatures 3D tensor as input and data in this case\n",
    "### was only 2D\n",
    "\n",
    "train_df = torch.tensor(train_df.values).unsqueeze(2)\n",
    "test_df = torch.tensor(test_df.values).unsqueeze(2)\n",
    "validation_df = torch.tensor(validation_df.values).unsqueeze(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"INITIALIZING MODEL\")\n",
    "### Initialize pipeline with dataset\n",
    "pipe = Pipeline.DTMLPipeline()\n",
    "pipe.input_data( [train_df, train_df,validation_df, validation_df, test_df, test_df])\n",
    "\n",
    "### Create DTML model within pipeline ank make sure all parameters are float\n",
    "pipe.create_model(params)\n",
    "pipe.model = pipe.model.float()\n",
    "print(\" The model has \" + str(Pipeline.count_parameters(pipe.model)) + \" parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model \n",
    "pipe.train_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot loss history\n",
    "pipe.plot_history()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Retrieve Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict and plot predictions\n",
    "y_pred, y_test = pipe.predict() \n",
    "### plot_predict(i) plots prediction vs result of stock nb 1 in the list\n",
    "pipe.preds = torch.mul(pipe.preds,  s) + torch.tensor(mu) \n",
    "pipe.tests = torch.mul(pipe.tests, s) + torch.tensor(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.plot_predict(stock_to_predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of the issues and tentative fixes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Description of \"Models.py\"\n",
    "\n",
    "    The file \"Models.py\" contains all the different models that are combined into the DTML model. The fourmain classes to look at are : \n",
    "    _ The \"contextEncoder\" class. It is a layer that encodes a panel of time series into a context matrix by running a LSTM over all lags of each series, and applies an attention layer as well as a custom layer norm to its output. The custom layer normalization is implemented in the class \"ContextNorm\" present in the same file. \n",
    "    The structure is Linear Layer -> Activation -> LSTM over all stocks/lags -> stacking hidden states into context matrix -> Attention Layer -> Context (Layer) Normalization\n",
    "\n",
    "    _ The \"contextAggregator\" class. It combines the output from two contextEncoders, one applied to all stocks and one applied to a market index (S&P 500) into a single context matrix. \n",
    "\n",
    "    _ The DASATransformer class is our decoder. It applies a multi-head attention layer to the context matrix and implements a residual connexion.\n",
    "\n",
    "    _the \"DTMLModel\" class, which combines the three above into the complete model. The structure of DTMLModel is : \n",
    "    Split data into stocks and market index -> Run a context encoder on each -> combine the context matrices into one with the contextAggregator -> Apply the DASATransformer -> Apply a Multi-layer perceptron -> output prediction\n",
    "\n",
    "2.Description of \"Pipeline.py\"\n",
    "\n",
    "    The main class is DTMLPipeline. has several methods that allow to run the model end to end : \n",
    "    _ __init__ initializes the class \n",
    "    _ input_data inputs the dataset (in train val test format)\n",
    "    _ create_model intitializes the model using our parameters\n",
    "    _ train_model trains the model. It creates the data loading structures, a warm-up learning rate scheduler and the model iteration over train and validation set\n",
    "    _ plot_history() plots the history of validation and training losses throughout the training\n",
    "    _ predict() runs the model on the test set and outputs the results \n",
    "    _plot_predict(n) plots the forecast and true data for the stock of index n.\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Description of issues and potential fixes\n",
    "\n",
    "    The main issue that we have faced while implementing this model has to do with training. Initially, all gradients in the encoding part of the model were vanishing, which is a common problem when having deep layers followed by attention layers. This is why we carefully implemented a residual connection as well as several layer normalizations in the Decoder (DASATransformer class). While this fixed the vanishing gradients in the encoding part of the network, the gradients were still vanishing in the multi-head attention block of the decoder. \n",
    "    We were then confronted with convergence issues, with the model converging to very different outcomes every run or simply not converging at all, producing swings in losses over the training. To remedy this, we used the paper \"Understainding the difficulty of training transformers\" (Liu 2020). We implemented several ideas : trying pre-LN normalization did not fix any convergence issue, but changing the optimizer to Adam as well as implementing a warm-up learning rate scheduler did fix the instability. As you can see if you run the model, the losses should be smooth throughout the training. \n",
    "    We still could not find how to fix the vanishing gradient problem within the attention layer of the decoder, and hence we produce a lot of flat forecasts. Another issue is that this model contains a lot of parameters, so the weekly dataset on only prices may be limiting its performance significantly as Transformers models need a lot of data to train properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLForFinance.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
